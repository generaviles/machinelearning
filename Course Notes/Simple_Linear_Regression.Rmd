---
title: " **2 Simple Linear Regression** "
output: html_notebook
---
<br \>


A linear regression (LR) can be mathematically expressed with the ***Estimated Regression Equation (ERE)***, also called the regression equation:

$$ y = b_0 + b_1x$$
Where:
  
  * $y$ is the **dependent variable (DV)** or the estimated value of the response value.
  * $x$ is the **independent variable (IV)**, a direct or associated cause influencing the result of the *DV*.
  * $b_1$ is the **coefficient**, it indicates the *slope* of the regression line. It tells us how a unit change in $x_1$ affects a change in $y$.
  * $b_0$ is the **constant**, the *y-intercept* of the regression line.<br \>
  
  * $b_0$ and $b_1$, together are called the *regression factors*.

A practical example of the application of a *LM*: if a boss wants to know how years in the work force affects salary to improve the way he set's payments for new workers. Salary would be the *DV* therefore represented on the *y* axis, experience the *IV* on the *x* axis. The constant is obtaind finding the place where the *fitting line* crosses the vertical axis. In our example this can be interpreted as follows:<br \>

$$\text{When years of experience} = 0. \text{ Then the salary should be} = a.$$
Where $a=$ the value of $b$ that intercepts the vertical axis (*y-intercept*).

$b_1$ is interpreted as the slope of the line in this example and it helps us answer the question: "How much more money should an employee get for every year of experience added?"<br \>

**The following image helps understand these concepts:**
![](D:\Dropbox\Clases\Udemy\Machine Learning A-Z\machinelearning/slr.png)

## How Does a Simple Linear Regression Model Works?

<br \>
A SLR model uses the **Theory of Least Squears (*Teoría de Mínimos Cuadrados*)**. <br \>
<br \>
What happens in the process is that a model is generated and all the values of $X$ are ploted and a projected line set, it starts calculating the distance of the real value ($y_i$) to the one projected by the model ($y_i'$), once it does that it sums the squared difference of each one of them and selects the minimum value, with it it generates the *projected line*.

**The following image explains this process:**

![](D:\Dropbox\Clases\Udemy\Machine Learning A-Z\machinelearning/slr1.png)

## Implementing Simple Linear Regression (SLR) in R

### Getting Things Ready
First of all, the working directory has to be pointing to where the dataset is, then the *data preprocessing* follows. It can be done using a template, here we use the one provided in the course and make little adjustment:

```{r}
setwd("D:/Dropbox/Clases/Udemy/Machine Learning A-Z/Part 2 - Regression/Section 4 - Simple Linear Regression")

# Data Preprocessing template

# Importing the dataset
dataset = read.csv('Salary_Data.csv')
# dataset = dataset[, 2:3]

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
# training_set[, 2:3] = scale(training_set[, 2:3])
# test_set[, 2:3] = scale(test_set[, 2:3])
```

### Fitting SLR to Training Set

```{r}
regressor <- lm(formula = Salary ~ YearsExperience,
                data = training_set)

summary(regressor)
```

Once the `regressor` is generated and a summary of it is printed, the *Coefficients* section of the summary can be use as an initiall source of information on the statistical correlation of the IV with the DV. Usually a $P$ value $>0.05$ is used to say that the statistical correlation is significant.<br \>
In this case $P = 1.52^{-14}$, which is much smaller than $0.05$ or $5\text{%}$, meaning that  `YearsExperience` and `Salary` are **strongly correlated**.

### Predicting the Test Set Results

First it is important to creat a vector of prediction, which will contain the predicted values of the `test_set` observations:

```{r}
y_pred <- predict(regressor, newdata = test_set)

```

With the creation of `y_pred` the model has already generated values for `Salary` that can be compared with the results of real life contained in `test_set`.

### Visualising the Training Set Results

First we will plot the `trining_set` results.

```{r}
library(ggplot2)
# The plotting will be done step by step

ggplot() +
  geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),
             colour = 'red') + 
  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)),
            colour = 'blue') + 
  ggtitle('Salary vs Experience (Training Set)') + 
  xlab('Years of Experience') + 
  ylab('Salary')
```


### Visualising the Test Set Results

Now it´s time to plot the `test_set` results.

```{r}
library(ggplot2)
# The plotting will be done step by step
ggplot() +
  geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
             colour = 'red') + 
  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)),
            colour = 'blue') + 
  ggtitle('Salary vs Experience (Test Set)') + 
  xlab('Years of Experience') + 
  ylab('Salary')
```