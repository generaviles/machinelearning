---
title: "Machine Learning Course Getting Things Ready"
output: html_notebook
---

This **Notebook** is my place for notes and practical applications for the course on [Machine Learning](https://www.udemy.com/machinelearning/learn/v4/overview) by [Kirill Eremenko](https://www.linkedin.com/in/keremenko), later I will produce another one with excercises from the CETYS project by the [group](https://da4ih.github.io) I am a member of.

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 


## **1. Data Preprocessing**

### 1.1 Importing the dataset
To import the data set one has to do the following:  

* Find the directory where the dataset is located at.
* Make that directory the "working directory".
```{r}
setwd("D:/Dropbox/Clases/Udemy/Machine Learning A-Z/machinelearning/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------")

dataset = read.csv('Data.csv')

dataset
```


It is important to remember that in **R** indexing starts at 1 instead of 0, which is the case in **Python**.

### 1.2 Dealing With Missing Data

When facing missing data in entries one could think of 2 approaches up till now:

1. Delete all entries with missing data. (**This is not cool and very very dangerous**, cause it could eliminate data that could influence the end result).
1. Calculate the ***mean*** according of all the data in the same variable from the other entries and replace the missing value.
    + This is the approach that will be used in this analysis.

#### 1.2.1 Dealing with the **Age** Column
```{r}
dataset$Age = ifelse(is.na(dataset$Age),
                     ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
                     dataset$Age)
dataset
```

#### 1.2.2 Dealing with the **Salary** Column

```{r}
dataset$Salary = ifelse(is.na(dataset$Salary),
                     ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
                     dataset$Salary)
dataset
```

Function **`ifelse`** takes three arguments:  

1. Condition.
2. Value wanted to be inputed if the condition is true.
3. Value wanted to be inputed if the condition is false.

### 1.3 Categorical Variables

Values that are reported with letters or in numbers not keeping it´s aritmethical value and relationships are called **categorical**, because they indicate *categories*. This type of data cannot be used *"as is"* in analysis with numerical values because it would generate errors. In order to be able to run analisys of these variables we have to ***encode*** them, which means to assign a numerical value to each of the categories.

**This is achieved in R by factoring the values of the variables with categorical data**

  - **Country Variable**
```{r}
dataset$Country = factor(dataset$Country,
                         levels = c('France','Spain','Germany'),
                         labels = c(1, 2, 3))
dataset
```

  - **Purchased Variable**

```{r}
dataset$Purchased = factor(dataset$Purchased,
                         levels = c('No','Yes'),
                         labels = c(0, 1))
dataset
```

### 1.4 Splitting Dataset into Training and Test Set

**For this process it is important to install and load `caTools`. `r library(caTools)`**

```{r}
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
```

This will generate a vector with results indicating either `True` or `False`, if true then it means that it has been chosen to be part of the *training* set, if false it means it has been chosen to be part of the *test* set.

Now the training and test sets have to be separated so we can work on both independently, the following code get's this acomplished:

```{r}
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

```
  
You can look at the **training_set** here:
```{r}
training_set
```

And the **test_set** here:
```{r}
test_set

```

### 1.5 Feature Scaling

I have to add the concetps of Eucledian Distance, Normalization and Standardization.

#### 1.5.1 Euclidean Distance
Many ML models are based on the concept of Euclidean Distance (EC), which can be defined as the squared root of the sum of the squared coordinates. The EC between two points can be mathematically dffined as: 
$$\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$$
When the range of values between variables is not similar the EC distance will be dominated by the variable with the broader range of values throwing off the model. This is why it is very important to **scale** variables when this happens so their valius will be in the similar ranges.

#### 1.5.2 Feature Scaling

There are 2 ways of achieving this:

  * Standardisation:

$$x_{stand} = \frac{x -mean(x)}{standard \ deviation(x)} $$
  
  * Normalisation:
  
$$x_{norm} = \frac{x-min(x)}{max(x)-min(x)} $$

```{r}
training_set[, 2:3] = scale(training_set[, 2:3])
test_set[, 2:3] = scale(test_set[, 2:3])
```

It is important to notice that only columns 2 and 3 were included in the feature scaling process because these are the only variables containing numerical values.

You can see an example of the result of the process in the `test_set`:
```{r}
test_set
```

# **2. Linear Regression**


A linear regression (LR) can be mathematically expressed as follows:

$$ y = b_0 + b_1 * x_1$$
Where:
  
  * $y$ is the **dependent variable (DV)** or what we are tryging to explain with the *LR*.
  * $x$ is the **independent variable (IV)** or a direct or associated cause influencing the result of the *DV*.
  * $b_1$ is the **coefficient** it tells us how a unit change in $x_1$ affects a change in $y$. It can be thought of the *connector* between $x$ and $y$.
  * $b_0$ is the **constant**.

A practical example of the application of a *LM*: if a boss wants to know how years in the work force affects salary to improve the way he set's payments for new workers. Salary would be the *DV* therefore represented on the *y* axis, experience the *IV* on the *x* axis. The constant is obtaind finding the place where the *fitting line* crosses the vertical axis. In our example this can be interpreted as follows:<br \>

$$\text{When years of experience} = 0. \text{ Then the salary should be} = a.$$
Where $a=$ the value on the vertical axis.

$b_1$ is interpreted as the slope of the line in this example and it helps us answer the question: "How much more money should an employee get for every year of experience added?"<br \>

**The following image helps understand these concepts:**
![](D:\Dropbox\Clases\Udemy\Machine Learning A-Z\machinelearning/slr.png)

## How Does a Simple Linear Regression Model Works?

<br \>
A SLR model uses the **Theory of Least Squears (*Teoría de Mínimos Cuadrados*)**. <br \>
<br \>
What happens in the process is that a model is generated and all the values of $X$ are ploted and a projected line set, it starts calculating the distance of the real value ($y_i$) to the one projected by the model ($y_i'$), once it does that it sums the squared difference of each one of them and selects the minimum value, with it it generates the *projected line*.

**The following image explains this process:**

![](D:\Dropbox\Clases\Udemy\Machine Learning A-Z\machinelearning/slr1.png)

## Implementing Simple Linear Regression (SLR) in R

### Getting Things Ready
First of all, the working directory has to be pointing to where the dataset is, then the *data preprocessing* follows. It can be done using a template, here we use the one provided in the course and make little adjustment:

```{r}
setwd("D:/Dropbox/Clases/Udemy/Machine Learning A-Z/Part 2 - Regression/Section 4 - Simple Linear Regression")

# Data Preprocessing template

# Importing the dataset
dataset = read.csv('Salary_Data.csv')
# dataset = dataset[, 2:3]

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
# training_set[, 2:3] = scale(training_set[, 2:3])
# test_set[, 2:3] = scale(test_set[, 2:3])
```

### Fitting SLR to Training Set

```{r}
regressor <- lm(formula = Salary ~ YearsExperience,
                data = training_set)

summary(regressor)
```

Once the `regressor` is generated and a summary of it is printed, the *Coefficients* section of the summary can be use as an initiall source of information on the statistical correlation of the IV with the DV. Usually a $P$ value $>0.05$ is used to say that the statistical correlation is significant.<br \>
In this case $P = 1.52^{-14}$, which is much smaller than $0.05$ or $5\text{%}$, meaning that  `YearsExperience` and `Salary` are **strongly correlated**.

### Predicting the Test Set Results

First it is important to creat a vector of prediction, which will contain the predicted values of the `test_set` observations:

```{r}
y_pred <- predict(regressor, newdata = test_set)

```

With the creation of `y_pred` the model has already generated values for `Salary` that can be compared with the results of real life contained in `test_set`.

### Visualising the Training Set Results

First we will plot the `trining_set` results.

```{r}
library(ggplot2)
# The plotting will be done step by step

ggplot() +
  geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),
             colour = 'red') + 
  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)),
            colour = 'blue') + 
  ggtitle('Salary vs Experience (Training Set)') + 
  xlab('Years of Experience') + 
  ylab('Salary')
```


### Visualising the Test Set Results

Now it´s time to plot the `test_set` results.

```{r}
library(ggplot2)
# The plotting will be done step by step
ggplot() +
  geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
             colour = 'red') + 
  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)),
            colour = 'blue') + 
  ggtitle('Salary vs Experience (Test Set)') + 
  xlab('Years of Experience') + 
  ylab('Salary')
```

<!--
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. -->

<!--```{r}
plot(cars)-->


<!-- Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).-->
